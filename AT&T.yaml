
---

## ğŸ”§ Technologies

- Python 3.10  
- TensorFlow / Keras  
- HuggingFace Transformers  
- Scikit-Learn  
- Matplotlib & Seaborn  

---

## ğŸš€ Models

### Baseline (BiLSTM)
- Embedding + BiLSTM + Dense
- High accuracy: **98%**
- AUC: **0.99**

### DistilBERT (Fine-Tuned)
- State-of-the-art NLP architecture
- Accuracy: **99%**
- AUC: **0.999**
- Low False Positives (3 only)

---

## ğŸ“Š Key Results

- DistilBERT significantly outperforms traditional Deep Learning models
- Excellent separation between ham / spam
- Ready for production-level deployment

---

## ğŸ“ Next Steps

- Deployment using FastAPI  
- Monitoring & automated retraining  
- Threshold optimization based on business risks  
- Scaling using GPU inference  

---

## ğŸ‘¨â€ğŸ’» Author
Faycel â€” Data Scientist @ Jedha